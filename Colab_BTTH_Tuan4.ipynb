{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Colab_BTTH_Tuan4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7goZQbiLvDx",
        "colab_type": "text"
      },
      "source": [
        "### Bài tập 1. Cho hàm số một biến $f(x) = x^2 + 4x + 3$\n",
        "\n",
        "1. Chúng ta sẽ thấy ngay hàm số đạt cực tiểu tại điểm $x^* = −2$. Bạn hãy giải thích tại sao bằng toán học. (*Gợi ý: Các bạn có thể dùng kiến thức môn Giải Tích đã học tại chương trình THPT*).\n",
        " * Thực hiện đạo hàm phương trình $f(x) = x^2 + 4x + 3$ ta được $f'(x) = 2x + 4$\n",
        " * Cho $f'(x) = 0$ ta được: $2x + 4 = 0 => x = -2$\n",
        " ![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rBmg_y856zO",
        "colab_type": "text"
      },
      "source": [
        "2. Viết chương trình máy tính sử dụng thuật toán Gradient Descent tìm giá trị gần đúng của $x^*$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PACYH-nR0sZ6",
        "colab_type": "code",
        "outputId": "4196a9ad-4975-4d52-a1e0-d8731bd8cd9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "from sympy import *\n",
        "print('Import libraries successfully')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Import libraries successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHk6sT4X52BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradient descent to find minimum of \n",
        "# derivative equation \n",
        "def gradient(learningRate, numIter, initX, theta):\n",
        "  minX = initX\n",
        "  theta = np.poly1d(theta) \n",
        "  for i in range(numIter): \n",
        "    derivTheta = theta.deriv() # Derivative the equation\n",
        "    minX = minX - learningRate*(derivTheta(minX))\n",
        "  return minX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzk-DDmHwJzq",
        "colab_type": "code",
        "outputId": "c696170c-76fd-4066-84b8-fa751f854d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learningRate = 0.01 \n",
        "numIter = 1000 \n",
        "initX = 0 \n",
        "theta = [1,4,3] # f(x) = x^2 + 4x + 3 \n",
        "print(gradient(learningRate,numIter, initX, theta))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.9999999966340651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qgy1eg_K8-k",
        "colab_type": "text"
      },
      "source": [
        "3. Thay đổi nghiệm (điểm cực tiểu) ban đầu khi thực hiện thuật toán Gradient Descent và nhận xét."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pr_WIWILVIo",
        "colab_type": "code",
        "outputId": "c0931006-8399-4ccc-a07b-c268789aa040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "initX = [-100,-10,0,10,100]\n",
        "for val in initX: \n",
        "  print(gradient(learningRate, numIter, val, theta))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-2.0000001649308015\n",
            "-2.000000013463738\n",
            "-1.9999999966340651\n",
            "-1.9999999798043921\n",
            "-1.9999998283373297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bRLbuFpLnJn",
        "colab_type": "text"
      },
      "source": [
        "##### Nhận xét: \n",
        "* Với các giá trị khác nhau dù âm hay dương, độ lớn của từng giá trị của điểm khởi đầu như thế nào nhưng tất cả kết quả trả về đều được kết quả gần đúng là 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiHMUbOALJUw",
        "colab_type": "text"
      },
      "source": [
        "4. Bạn có thể chạy thuật toán Gradient Descent bằng tay cho $2$ vòng lặp được không?\n",
        "##### Phương trình $f(x) = x^2 + 4x + 3$\n",
        "##### Ta chọn ${\\eta} = 0.01$ và $x = 0$\n",
        "##### Gradient descent: $x = x - {\\eta}f'(x)$\n",
        "##### Vòng lặp:\n",
        "* Vòng lặp đầu tiên\n",
        " * Thay $x = 0$ vào $f'(x) = 2x + 4$ ta được $f'(0) = 4$\n",
        " * $x = x - {\\eta}f'(x) =>  x = 0 - 0.01*4 = -0.04$\n",
        "* Vòng lắp thứ hai: \n",
        " * Thay $x = -0.04 $ vào $f'(-0.04) = 3.92$\n",
        " * $x = x - {\\eta}f'(x) =>  x = -0.04 - 0.01*3.92 = -0.0792$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5DK1SHhLvDz",
        "colab_type": "text"
      },
      "source": [
        "### Bài tập 2. Cho hàm số hai biến $f(x_1, x_2) = x_1^2 + 3x_2^2 + 1$\n",
        "\n",
        "1. Chúng ta sẽ tìm được một điểm cực trị $x^* = (0, 0)$. Bạn hãy giải thích tại sao bằng toán học (*Gợi ý: Các bạn có thể dùng kiến thức môn Giải Tích đã học tại chương trình Đại học*).\n",
        "* Ta thực hiện đạo hàm \n",
        " * $f_{x1} = 2x_1$,  $f_{x1x1} = 2$\n",
        " * $f_{x2} = 6x_2$,  $f_{x2x2} = 6$\n",
        " * $f_{x1x2} = 0$\n",
        "* Tìm nghiệm\n",
        " * $f_{x1} = 2x_1 = 0 => x_1 = 0$\n",
        " * $f_{x2} = 6x_2 = 0 => x_2 = 0 $\n",
        "#### $=>$ Nghiệm tìm được $(0,0)$  \n",
        "* Đánh giá nghiệm vừa tìm được: \n",
        " * $D = f_{x1x1}(0,0)f_{x2x2}(0,0)-[f_{x1x2}(0,0)]^2 = 12 $  \n",
        " $=> D > 0$ và $f_{x1x1} > 0$ nên điểm cực tiểu tìm được là $(0,0)$ \n",
        "4. Bạn có thể chạy thuật toán Gradient Descent bằng tay cho $2$ vòng lặp được không?\n",
        "* $x = x - {\\nabla}f(x)$ và giả sử cho $(x1,x2) = (-1,-1)$, {\\nabla} = 0.01\n",
        "* Vòng lặp đầu tiên:\n",
        " * $x1 = x1 - {\\nabla}*f_{x1} = -1 - 0.01*(2*-1) = -0.98$\n",
        " * $x2 = x2 - {\\nabla}*f_{x2} = -1 - 0.01*(6*-1) = -0.94$ \n",
        "* Vòng lặp thứ 2:\n",
        " * $x1 = x1 - {\\nabla}*f_{x1} = -0.98 - 0.01*(2*-0.98) = -0.9604$\n",
        " * $x2 = x2 - {\\nabla}*f_{x2} = -0.94 - 0.01*(6*-0.94) = -0.88359$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyIGDWCiajlV",
        "colab_type": "text"
      },
      "source": [
        "2. Viết chương trình máy tính sử dụng thuật toán Gradient Descent tìm giá trị gần đúng của $x^*$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFhlIunnaqZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# f(x,y) = x1^2 + 3*x2^2 + 1\n",
        "devTheta1 = 2 # f(x,y) after deriviation\n",
        "devTheta2 = 6 # f(x,y) after deriviation\n",
        "theta = [devTheta1, devTheta2] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbqaUJCxNJH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multiGradient(learningRate, numIter, theta):\n",
        "  for iterations in range(numIter): \n",
        "    theta[0] = theta[0] - learningRate*(theta[0])\n",
        "    theta[1] = theta[1] - learningRate*(theta[1])\n",
        "  return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki-_nrJ79uZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b70ca5f1-e074-4320-dbee-20da338f2f83"
      },
      "source": [
        "learningRate = 0.01\n",
        "numIter = 1000\n",
        "# f(x,y) = x1^2 + 3*x2^2 + 1\n",
        "devTheta1 = -2 # f(x,y) after deriviation\n",
        "devTheta2 = -4 # f(x,y) after deriviation\n",
        "theta = [devTheta1, devTheta2] \n",
        "print(multiGradient(learningRate,numIter, theta))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-8.634249482131659e-05, -0.00017268498964263318]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRKhVjzjaoTN",
        "colab_type": "text"
      },
      "source": [
        "3. Thay đổi nghiệm (điểm cực tiểu) ban đầu khi thực hiện thuật toán Gradient Descent và nhận xét.\n",
        "* Nhận xét: \n",
        " * Mặc dù thay đổi nghiệm (điểm cực tiểu) ban đầu khi thực hiện thuật toán Gradient Descent nhưng kết quả vẫn không thay đổi, vẫn trả về kết quả mà ta đã thực hiện tính toán trước đó"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sETvs6CLvD0",
        "colab_type": "text"
      },
      "source": [
        "### Bài tập 3. Thuật toán Batch Gradient Descent\n",
        "1. Cài đặt thuật toán Batch Gradient Descent cho bài toán Linear Regression trên dữ liệu `data-demo.xls` bằng cách sử dụng chương trình Python tại trang `122` và in ra màn hình kết quả của `theta`.\n",
        "\n",
        "        eta = 0.1 # learning rate\n",
        "        n_iterations = 1000\n",
        "        m = 100\n",
        "        theta = np.random.randn(2,1) # random initialization\n",
        "        for iteration in range(n_iterations):\n",
        "            gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "            theta = theta - eta * gradients\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5hWRCD5SDRg",
        "colab_type": "code",
        "outputId": "20348463-5362-410e-dc09-7a0ff9824db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "print('Import libraries successfully')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Import libraries successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yGycEn4TWsR",
        "colab_type": "code",
        "outputId": "2a6d0fa1-29ff-460d-b7df-8c72aad1015a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read file\n",
        "data = pd.ExcelFile('demo_data.xls')\n",
        "df = pd.read_excel(data,0,header = 0)\n",
        "df.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXHZZI-9vKUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchGradient(n_iterations, eta, theta, X_b, y): \n",
        "  for iteration in range(n_iterations):\n",
        "     gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "     theta = theta - eta * gradients\n",
        "  return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSp_raOpgAkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df['X']\n",
        "X = X.to_numpy()\n",
        "X = np.reshape(X, (100,1))\n",
        "X_b = np.c_[np.ones((100,1)), X]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuT11SnXux9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = df['y']\n",
        "y = y.to_numpy()\n",
        "y = np.reshape(y, (100, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kKuUcpfwPvj",
        "colab_type": "code",
        "outputId": "a5a583c5-4a80-42bd-9930-051bfdd70315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Setting parameter\n",
        "eta = 0.1 # learning rate\n",
        "n_iterations = 1000\n",
        "m = 100\n",
        "theta = np.random.randint(100, size = (2,1)) # random initialize theta\n",
        "theta_best = batchGradient(n_iterations,eta,theta,X_b,y)\n",
        "theta_best"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.1648496 ],\n",
              "       [2.93275231]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwcvW43NxMsj",
        "colab_type": "text"
      },
      "source": [
        "2. Bỏ hằng số $2$ trong biểu thức $(2/m)$, sau đó chạy chương trình đã đề cập ở `Bài tập 3.1.` và nhận xét."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtU9T4taxNer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchGradient_WIHTOUT2(n_iterations, eta, theta, X_b, y): \n",
        "  for iteration in range(n_iterations):\n",
        "     gradients = 1/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "     theta = theta - eta * gradients\n",
        "  return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVKkrFkvxqv1",
        "colab_type": "code",
        "outputId": "b9b6fee5-71ea-4aff-a7ef-0b00faca2eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "theta_best_2 = batchGradient_WIHTOUT2(n_iterations,eta,theta,X_b,y)\n",
        "theta_best_2"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.16485092],\n",
              "       [2.93275117]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqS4GdqwyDnF",
        "colab_type": "text"
      },
      "source": [
        "Nhận xét: \n",
        "* Dù bỏ đi $2$ nhưng giá trị của $\\hat\\theta$ được trả về vẫn không đổi\n",
        "* Gradient descent vẫn tìm được $\\hat\\theta$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiTDrzeqxR4m",
        "colab_type": "text"
      },
      "source": [
        "3. Bỏ số tham số $m$ trong biểu thức $(2/m)$, sau đó chạy chương trình đã đề cập ở `Bài tập 3.1.` và nhận xét.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNNHzHW0ya4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchGradient_NO_AVG(n_iterations, eta, theta, X_b, y): \n",
        "  for iteration in range(n_iterations):\n",
        "     gradients = 1/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "     theta = theta - eta * gradients\n",
        "  return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpLJuKz4yihC",
        "colab_type": "code",
        "outputId": "119d7934-0690-47ac-a440-53a3a22190cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "theta_best_NO_AVG = batchGradient_NO_AVG(n_iterations,eta,theta,X_b,y)\n",
        "theta_best_NO_AVG"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.16485092],\n",
              "       [2.93275117]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBM5uyzoyomL",
        "colab_type": "text"
      },
      "source": [
        "Nhận xét:\n",
        "* Dù bỏ đi $2/m$ dùng để tính giá trị trung bình nhưng thuật toán batch gradient descent vẫn tìm ra được $\\hat\\theta$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvCgNARRybmN",
        "colab_type": "text"
      },
      "source": [
        "4. Thay đổi giá trị của `eta` (ký hiệu toán học $\\eta$: độ học), giá trị khởi tạo của `theta` có sự khác biệt so với ban đầu và nhận xét kết quả đạt được.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M77GovmFy_cK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchGradient(n_iterations, eta, theta, X_b, y): \n",
        "  for iteration in range(n_iterations):\n",
        "     gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "     theta = theta - eta * gradients\n",
        "  return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGO7Sk9uzGHL",
        "colab_type": "code",
        "outputId": "714ffcc3-93eb-4584-9e50-ac5b239d918b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Setting parameter\n",
        "etaList = [0.001,0.01, 0.1, 1, 10] # list of learning rate\n",
        "n_iterations = 1000\n",
        "m = 100\n",
        "theta = np.random.randint(100, size = (2,1)) # random initialize theta\n",
        "for eta in etaList: \n",
        "  temp = batchGradient(n_iterations, eta, theta, X_b,y)\n",
        "  print(temp)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-3.0115867 ]\n",
            " [10.64715947]]\n",
            "[[3.78988074]\n",
            " [3.2558015 ]]\n",
            "[[4.1648496 ]\n",
            " [2.93275231]]\n",
            "[[nan]\n",
            " [nan]]\n",
            "[[nan]\n",
            " [nan]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in subtract\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th-pQCc8MJS8",
        "colab_type": "text"
      },
      "source": [
        "Nhận xét: \n",
        "* Khi độ học (learning rate) càng bé, thì thuật toán gradient descent càng chính xác hơn trong việc tìm $\\hat\\theta$ \n",
        "* Khi độ học (learning rate) lớn, có thể khiến cho việc tìm kiếm $\\hat\\theta$ lâu hơn hoặc có thể không tìm ra kết quả (trong trường hợp này là $\\eta = 0.1, 1, 10$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XrRF_EIMLd1",
        "colab_type": "code",
        "outputId": "4986537a-9a0f-4f2a-98bc-29cfc2f6d84a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "# Setting parameter\n",
        "eta = 0.01 # learning rate\n",
        "n_iterations = 1000\n",
        "m = 100\n",
        "for i in range(4): \n",
        "  theta = np.random.randint(100, size = (2,1)) # random initialize theta\n",
        "  temp = batchGradient(n_iterations, eta, theta, X_b,y)\n",
        "  print(\"theta value: \",theta)\n",
        "  print(\"Gradient return: \",temp)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "theta value:  [[ 4]\n",
            " [55]]\n",
            "Gradient return:  [[3.29427258]\n",
            " [3.68278581]]\n",
            "theta value:  [[72]\n",
            " [87]]\n",
            "Gradient return:  [[4.07606034]\n",
            " [3.00924746]]\n",
            "theta value:  [[99]\n",
            " [63]]\n",
            "Gradient return:  [[4.99796017]\n",
            " [2.21499751]]\n",
            "theta value:  [[96]\n",
            " [63]]\n",
            "Gradient return:  [[4.93995091]\n",
            " [2.26497458]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyeOz3UezMq3",
        "colab_type": "text"
      },
      "source": [
        "Nhận xét: \n",
        "* Dù ta có thay đổi giá trị $\\theta$ như thế nào nhưng thuật toán gradient descent vẫn tìm được $\\hat\\theta$\n",
        "* Giá trị $\\theta$ được đặt khác nhau nhưng nhìn chung gradient descent trả về cho ta kết quả tương đối gần nhau "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zyzkfx2y-kZ",
        "colab_type": "text"
      },
      "source": [
        "5. Thay đổi điều kiện dừng bằng cách sử dụng điều kiện `gradient` < `epsilon`. (Gợi ý: Các bạn khởi tạo biến `epsilon` có giá trị thật nhỏ bằng dòng lệnh, ví dụ `epsilon = 1e-5` tương đương với epsilion = 0.00001).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gntPcNMz-B8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batchGradient_CONDITION(n_iterations, eta, epsilon, theta, X_b, y): \n",
        "  for iteration in range(n_iterations):\n",
        "     gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) \n",
        "     if gradients.any() >= epsilon:\n",
        "       break\n",
        "     theta = theta - eta * gradients\n",
        "  return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdjfcaeTO7d0",
        "colab_type": "code",
        "outputId": "94c36ecb-24f5-4d8f-d951-e4a92b5316f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "# Setting parameter\n",
        "eta = 0.001 # learning rate\n",
        "n_iterations = 1000\n",
        "m = 100\n",
        "\n",
        "for i in range(5): \n",
        "  epsilon = 10**(-i)\n",
        "  theta = np.random.randint(100, size = (2,1)) # random initialize theta\n",
        "  print('epsilon value:', epsilon)\n",
        "  print(\"Theta value: \",theta)\n",
        "  print(\"Gradient return: \",batchGradient_CONDITION(n_iterations,eta,epsilon,theta,X_b,y))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epsilon value: 1\n",
            "Theta value:  [[99]\n",
            " [31]]\n",
            "Gradient return:  [[99]\n",
            " [31]]\n",
            "epsilon value: 0.1\n",
            "Theta value:  [[14]\n",
            " [83]]\n",
            "Gradient return:  [[14]\n",
            " [83]]\n",
            "epsilon value: 0.01\n",
            "Theta value:  [[42]\n",
            " [59]]\n",
            "Gradient return:  [[42]\n",
            " [59]]\n",
            "epsilon value: 0.001\n",
            "Theta value:  [[ 4]\n",
            " [67]]\n",
            "Gradient return:  [[ 4]\n",
            " [67]]\n",
            "epsilon value: 0.0001\n",
            "Theta value:  [[80]\n",
            " [91]]\n",
            "Gradient return:  [[80]\n",
            " [91]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMBS0CdlVDpy",
        "colab_type": "text"
      },
      "source": [
        "Nhận xét: \n",
        "* Mặc dù thay đổi epsilon giảm dần nhưng kết quả lại nhận được rằng $\\theta = \\hat\\theta$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2yEVAFDLvD0",
        "colab_type": "text"
      },
      "source": [
        "### Bài tập 4. Thuật toán Stochastic Gradient Descent\n",
        "1. Cài đặt thuật toán Stochastic Gradient Descent cho bài toán Linear Regression trên dữ liệu `data-demo.xls` bằng cách sử dụng chương trình Python tại trang `125` và in ra màn hình kết quả của `theta`.\n",
        "\n",
        "        n_epochs = 50\n",
        "        t0, t1 = 5, 50 # learning schedule hyperparameters\n",
        "        def learning_schedule(t):\n",
        "            return t0 / (t + t1)\n",
        "        theta = np.random.randn(2,1) # random initialization\n",
        "        for epoch in range(n_epochs):\n",
        "            for i in range(m):\n",
        "                random_index = np.random.randint(m)\n",
        "                xi = X_b[random_index:random_index+1]\n",
        "                yi = y[random_index:random_index+1]\n",
        "                gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "                eta = learning_schedule(epoch * m + i)\n",
        "                theta = theta - eta * gradients\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBCDjNA6VlCD",
        "colab_type": "code",
        "outputId": "8443c203-4a9a-47dd-a9a9-45a69e76efa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "m = 100\n",
        "n_epochs = 50\n",
        "t0, t1 = 5, 50\n",
        "\n",
        "def learning_schedule(t):\n",
        "    return t0/(t + t1)\n",
        "\n",
        "theta = np.random.randn(2,1)\n",
        "\n",
        "for epoch in range (n_epochs):\n",
        "    for i in range (m):\n",
        "        random_index = np.random.randint(m)\n",
        "        # xi,yi already read from DataFrame \n",
        "        xi = X_b[random_index : random_index + 1]\n",
        "        yi = y[random_index : random_index + 1]\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(epoch * m + i)\n",
        "        theta = theta - eta * gradients\n",
        "\n",
        "print(theta)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4.16592211]\n",
            " [2.96463288]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUES0tYUXKMK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "2. So sánh nhanh hai chương trình Python đã đề cập ở `Bài tập 3.1.` và `Bài tập 4.1.`.\n",
        "* Ta nhận thấy rằng kết quả giữa Batch Gradient Descent và Stochastic Gradient Descent là tương đương nhau\n",
        "* Gradient descent áp dụng phương pháp tính trung bình cho đạo hàm nhưng Stochastic lại không sử dụng phương pháp đó nhưng lại cho kết quả tương đương"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scUe6SngXL2u",
        "colab_type": "text"
      },
      "source": [
        "3. Vai trò của hàm `learning_schedule` trong chương trình Python đã đề cập ở `Bài tập 4.1.`. Nếu không sử dụng hàm này thì kết quả của bài tập sẽ như thế nào.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuEoXI9EXvwF",
        "colab_type": "code",
        "outputId": "f14180fe-62d9-426f-cf49-ae90fc5ca638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "m = 100\n",
        "n_epochs = 50\n",
        "t0, t1 = 5, 50\n",
        "\n",
        "theta = np.random.randn(2,1)\n",
        "# Without learning schedule\n",
        "for epoch in range (n_epochs):\n",
        "    for i in range (m):\n",
        "        random_index = np.random.randint(m)\n",
        "        # xi,yi already read from DataFrame \n",
        "        xi = X_b[random_index : random_index + 1]\n",
        "        yi = y[random_index : random_index + 1]\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = 0.01 # Self-choose for eta\n",
        "        theta = theta - eta * gradients\n",
        "\n",
        "print(theta)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4.36908545]\n",
            " [2.92709264]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNAoVvS8XwUM",
        "colab_type": "text"
      },
      "source": [
        "Nhận xét:\n",
        "* Không áp dụng learning_schedule ta vẫn có thể cho kết quả tương đương như Batch Gradient Descent tuy nhiên có điểm cần lưu ý là phải chọn giá trị $\\eta$ sao cho hợp lí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4PAp-qXXO3s",
        "colab_type": "text"
      },
      "source": [
        "4. Sử dùng lớp [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) để cài đặt thuật toán Stochastic Gradient Descent thay cho chương trình Python đã đề cập ở `Bài tập 4.1.`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsTV72acYe26",
        "colab_type": "code",
        "outputId": "0b579eb2-d13c-48e6-c975-df03bd94ec1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "sgd_reg = SGDRegressor(max_iter = 100, tol = 1e-3, penalty = None, eta0=0.1)\n",
        "sgd_reg.fit(X,y)\n",
        "sgd_reg.intercept_, sgd_reg.coef_"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([4.16135289]), array([2.96175701]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYc9a1H9LvD1",
        "colab_type": "text"
      },
      "source": [
        "### Bài tập 5. Thuật toán Mini-Batch Gradient Descent\n",
        "1. Cài đặt thuật toán `Mini-Batch Gradient Descent` cho bài toán `Linear Regression` trên dữ liệu `data-demo.xls` bằng cách `chỉnh sửa` chương trình Python tại trang `125` và in ra màn hình kết quả của `theta`.\n",
        "\n",
        "        n_epochs = 50\n",
        "        t0, t1 = 5, 50 # learning schedule hyperparameters\n",
        "        def learning_schedule(t):\n",
        "            return t0 / (t + t1)\n",
        "        theta = np.random.randn(2,1) # random initialization\n",
        "        for epoch in range(n_epochs):\n",
        "            for i in range(m):\n",
        "                random_index = np.random.randint(m)\n",
        "                xi = X_b[random_index:random_index+1]\n",
        "                yi = y[random_index:random_index+1]\n",
        "                gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "                eta = learning_schedule(epoch * m + i)\n",
        "                theta = theta - eta * gradients\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSGMul7YY62R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eta = 0.1  \n",
        "theta = np.random.randn(2,1)  \n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)    \n",
        "    theta = theta - eta * gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XqUHe_qYr0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_mini_batches(X, y, batch_size): \n",
        "    mini_batches = [] \n",
        "    data = np.hstack((X, y)) \n",
        "    np.random.shuffle(data) \n",
        "    n_minibatches = data.shape[0] // batch_size \n",
        "    i = 0\n",
        "    for i in range(n_minibatches + 1): \n",
        "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :] \n",
        "        X_mini = mini_batch[:, :-1] \n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1)) \n",
        "        mini_batches.append((X_mini, Y_mini)) \n",
        "    if data.shape[0] % batch_size != 0: \n",
        "        mini_batch = data[i * batch_size:data.shape[0]] \n",
        "        X_mini = mini_batch[:, :-1] \n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1)) \n",
        "        mini_batches.append((X_mini, Y_mini)) \n",
        "    return mini_batches "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwb4IMPxZCr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def miniBatchGradient(X, y, learning_rate = 0.1, batch_size = 32): \n",
        "    theta = np.random.randint(100, size = (2,1))\n",
        "    max_iters = 3\n",
        "    for itr in range(100): \n",
        "        mini_batches = create_mini_batches(X, y, batch_size) \n",
        "        for mini_batch in mini_batches: \n",
        "            X_mini, y_mini = mini_batch \n",
        "            gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "            theta = theta - learning_rate * gradients\n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w2RwFs9VsYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c44d308c-f9eb-4bfc-d50c-62f71efa4929"
      },
      "source": [
        "print(miniBatchGradient(X,y, 0.1, 32))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4.16485052]\n",
            " [2.93275152]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Su5toBYsKH",
        "colab_type": "text"
      },
      "source": [
        "2. Sử dùng lớp [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) để cài đặt thuật toán Stochastic Gradient Descent thay cho chương trình Python đã đề cập ở `Bài tập 5.1.`. *Gợi ý*: Khác với yêu cầu trong `Bài tập 4.4.`, các bạn sẽ chia bộ dữ liệu huấn luyện ban đầu thành các bộ dữ liệu nhỏ có kích cỡ gọi là `Mini-Batch Size`, ví dụ các bạn có thể sử dụng $4$ chẳng hạn. Cuối cùng, thay vì các bạn sử dụng phương thức [`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor.fit) của [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) thì các bạn sẽ sử dụng phương thức [`partial_fit`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor.partial_fit) cho các bộ dữ liệu nhỏ thành phần."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPOiCOQVzu78",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e43c3de0-f2ea-4647-eca5-c843ac8804cc"
      },
      "source": [
        "from sklearn import linear_model\n",
        "print('Import linear model successfully')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Import linear model successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY1e60GXGaN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "miniSGD = SGDRegressor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQYB3Zd5Gaeq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "aa6e468d-9990-4679-a7e4-373ed9713ae4"
      },
      "source": [
        "miniSGD.partial_fit(X,y)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
              "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
              "             learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
              "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
              "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
              "             warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CcDuJC5Gald",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "295aae9d-9568-4238-c730-b0cacf702baf"
      },
      "source": [
        "miniSGD.intercept_, miniSGD.coef_"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.95774596]), array([2.16881122]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    }
  ]
}
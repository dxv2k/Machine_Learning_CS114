# -*- coding: utf-8 -*-
"""Colab_BTTH_Tuan4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1574_CQOLig7RcpBvi15luZc7S2R3DPZt

### Bài tập 1. Cho hàm số một biến $f(x) = x^2 + 4x + 3$

1. Chúng ta sẽ thấy ngay hàm số đạt cực tiểu tại điểm $x^* = −2$. Bạn hãy giải thích tại sao bằng toán học. (*Gợi ý: Các bạn có thể dùng kiến thức môn Giải Tích đã học tại chương trình THPT*).
 * Thực hiện đạo hàm phương trình $f(x) = x^2 + 4x + 3$ ta được $f'(x) = 2x + 4$
 * Cho $f'(x) = 0$ ta được: $2x + 4 = 0 => x = -2$
 ![image.png](data:image/png;base64,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)

2. Viết chương trình máy tính sử dụng thuật toán Gradient Descent tìm giá trị gần đúng của $x^*$.
"""

import numpy as np 
import matplotlib.pyplot as plt 
from sympy import *
print('Import libraries successfully')

# Gradient descent to find minimum of 
# derivative equation 
def gradient(learningRate, numIter, initX, theta):
  minX = initX
  theta = np.poly1d(theta) 
  for i in range(numIter): 
    derivTheta = theta.deriv() # Derivative the equation
    minX = minX - learningRate*(derivTheta(minX))
  return minX

learningRate = 0.01 
numIter = 1000 
initX = 0 
theta = [1,4,3] # f(x) = x^2 + 4x + 3 
print(gradient(learningRate,numIter, initX, theta))

"""3. Thay đổi nghiệm (điểm cực tiểu) ban đầu khi thực hiện thuật toán Gradient Descent và nhận xét."""

initX = [-100,-10,0,10,100]
for val in initX: 
  print(gradient(learningRate, numIter, val, theta))

"""##### Nhận xét: 
* Với các giá trị khác nhau dù âm hay dương, độ lớn của từng giá trị của điểm khởi đầu như thế nào nhưng tất cả kết quả trả về đều được kết quả gần đúng là 2

4. Bạn có thể chạy thuật toán Gradient Descent bằng tay cho $2$ vòng lặp được không?
##### Phương trình $f(x) = x^2 + 4x + 3$
##### Ta chọn ${\eta} = 0.01$ và $x = 0$
##### Gradient descent: $x = x - {\eta}f'(x)$
##### Vòng lặp:
* Vòng lặp đầu tiên
 * Thay $x = 0$ vào $f'(x) = 2x + 4$ ta được $f'(0) = 4$
 * $x = x - {\eta}f'(x) =>  x = 0 - 0.01*4 = -0.04$
* Vòng lắp thứ hai: 
 * Thay $x = -0.04 $ vào $f'(-0.04) = 3.92$
 * $x = x - {\eta}f'(x) =>  x = -0.04 - 0.01*3.92 = -0.0792$

### Bài tập 2. Cho hàm số hai biến $f(x_1, x_2) = x_1^2 + 3x_2^2 + 1$

1. Chúng ta sẽ tìm được một điểm cực trị $x^* = (0, 0)$. Bạn hãy giải thích tại sao bằng toán học (*Gợi ý: Các bạn có thể dùng kiến thức môn Giải Tích đã học tại chương trình Đại học*).
* Ta thực hiện đạo hàm 
 * $f_{x1} = 2x_1$,  $f_{x1x1} = 2$
 * $f_{x2} = 6x_2$,  $f_{x2x2} = 6$
 * $f_{x1x2} = 0$
* Tìm nghiệm
 * $f_{x1} = 2x_1 = 0 => x_1 = 0$
 * $f_{x2} = 6x_2 = 0 => x_2 = 0 $
#### $=>$ Nghiệm tìm được $(0,0)$  
* Đánh giá nghiệm vừa tìm được: 
 * $D = f_{x1x1}(0,0)f_{x2x2}(0,0)-[f_{x1x2}(0,0)]^2 = 12 $  
 $=> D > 0$ và $f_{x1x1} > 0$ nên điểm cực tiểu tìm được là $(0,0)$ 
4. Bạn có thể chạy thuật toán Gradient Descent bằng tay cho $2$ vòng lặp được không?
* $x = x - {\nabla}f(x)$ và giả sử cho $(x1,x2) = (-1,-1)$, {\nabla} = 0.01
* Vòng lặp đầu tiên:
 * $x1 = x1 - {\nabla}*f_{x1} = -1 - 0.01*(2*-1) = -0.98$
 * $x2 = x2 - {\nabla}*f_{x2} = -1 - 0.01*(6*-1) = -0.94$ 
* Vòng lặp thứ 2:
 * $x1 = x1 - {\nabla}*f_{x1} = -0.98 - 0.01*(2*-0.98) = -0.9604$
 * $x2 = x2 - {\nabla}*f_{x2} = -0.94 - 0.01*(6*-0.94) = -0.88359$

2. Viết chương trình máy tính sử dụng thuật toán Gradient Descent tìm giá trị gần đúng của $x^*$.
"""

# f(x,y) = x1^2 + 3*x2^2 + 1
devTheta1 = 2 # f(x,y) after deriviation
devTheta2 = 6 # f(x,y) after deriviation
theta = [devTheta1, devTheta2]

def multiGradient(learningRate, numIter, theta):
  for iterations in range(numIter): 
    theta[0] = theta[0] - learningRate*(theta[0])
    theta[1] = theta[1] - learningRate*(theta[1])
  return theta

learningRate = 0.01
numIter = 1000
# f(x,y) = x1^2 + 3*x2^2 + 1
devTheta1 = -2 # f(x,y) after deriviation
devTheta2 = -4 # f(x,y) after deriviation
theta = [devTheta1, devTheta2] 
print(multiGradient(learningRate,numIter, theta))

"""3. Thay đổi nghiệm (điểm cực tiểu) ban đầu khi thực hiện thuật toán Gradient Descent và nhận xét.
* Nhận xét: 
 * Mặc dù thay đổi nghiệm (điểm cực tiểu) ban đầu khi thực hiện thuật toán Gradient Descent nhưng kết quả vẫn không thay đổi, vẫn trả về kết quả mà ta đã thực hiện tính toán trước đó

### Bài tập 3. Thuật toán Batch Gradient Descent
1. Cài đặt thuật toán Batch Gradient Descent cho bài toán Linear Regression trên dữ liệu `data-demo.xls` bằng cách sử dụng chương trình Python tại trang `122` và in ra màn hình kết quả của `theta`.

        eta = 0.1 # learning rate
        n_iterations = 1000
        m = 100
        theta = np.random.randn(2,1) # random initialization
        for iteration in range(n_iterations):
            gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
            theta = theta - eta * gradients
"""

import numpy as np
import pandas as pd
print('Import libraries successfully')

# Read file
data = pd.ExcelFile('demo_data.xls')
df = pd.read_excel(data,0,header = 0)
df.shape

def batchGradient(n_iterations, eta, theta, X_b, y): 
  for iteration in range(n_iterations):
     gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
     theta = theta - eta * gradients
  return theta

X = df['X']
X = X.to_numpy()
X = np.reshape(X, (100,1))
X_b = np.c_[np.ones((100,1)), X]

y = df['y']
y = y.to_numpy()
y = np.reshape(y, (100, 1))

# Setting parameter
eta = 0.1 # learning rate
n_iterations = 1000
m = 100
theta = np.random.randint(100, size = (2,1)) # random initialize theta
theta_best = batchGradient(n_iterations,eta,theta,X_b,y)
theta_best

"""2. Bỏ hằng số $2$ trong biểu thức $(2/m)$, sau đó chạy chương trình đã đề cập ở `Bài tập 3.1.` và nhận xét."""

def batchGradient_WIHTOUT2(n_iterations, eta, theta, X_b, y): 
  for iteration in range(n_iterations):
     gradients = 1/m * X_b.T.dot(X_b.dot(theta) - y)
     theta = theta - eta * gradients
  return theta

theta_best_2 = batchGradient_WIHTOUT2(n_iterations,eta,theta,X_b,y)
theta_best_2

"""Nhận xét: 
* Dù bỏ đi $2$ nhưng giá trị của $\hat\theta$ được trả về vẫn không đổi
* Gradient descent vẫn tìm được $\hat\theta$

3. Bỏ số tham số $m$ trong biểu thức $(2/m)$, sau đó chạy chương trình đã đề cập ở `Bài tập 3.1.` và nhận xét.
"""

def batchGradient_NO_AVG(n_iterations, eta, theta, X_b, y): 
  for iteration in range(n_iterations):
     gradients = 1/m * X_b.T.dot(X_b.dot(theta) - y)
     theta = theta - eta * gradients
  return theta

theta_best_NO_AVG = batchGradient_NO_AVG(n_iterations,eta,theta,X_b,y)
theta_best_NO_AVG

"""Nhận xét:
* Dù bỏ đi $2/m$ dùng để tính giá trị trung bình nhưng thuật toán batch gradient descent vẫn tìm ra được $\hat\theta$

4. Thay đổi giá trị của `eta` (ký hiệu toán học $\eta$: độ học), giá trị khởi tạo của `theta` có sự khác biệt so với ban đầu và nhận xét kết quả đạt được.
"""

def batchGradient(n_iterations, eta, theta, X_b, y): 
  for iteration in range(n_iterations):
     gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
     theta = theta - eta * gradients
  return theta

# Setting parameter
etaList = [0.001,0.01, 0.1, 1, 10] # list of learning rate
n_iterations = 1000
m = 100
theta = np.random.randint(100, size = (2,1)) # random initialize theta
for eta in etaList: 
  temp = batchGradient(n_iterations, eta, theta, X_b,y)
  print(temp)

"""Nhận xét: 
* Khi độ học (learning rate) càng bé, thì thuật toán gradient descent càng chính xác hơn trong việc tìm $\hat\theta$ 
* Khi độ học (learning rate) lớn, có thể khiến cho việc tìm kiếm $\hat\theta$ lâu hơn hoặc có thể không tìm ra kết quả (trong trường hợp này là $\eta = 0.1, 1, 10$
"""

# Setting parameter
eta = 0.01 # learning rate
n_iterations = 1000
m = 100
for i in range(4): 
  theta = np.random.randint(100, size = (2,1)) # random initialize theta
  temp = batchGradient(n_iterations, eta, theta, X_b,y)
  print("theta value: ",theta)
  print("Gradient return: ",temp)

"""Nhận xét: 
* Dù ta có thay đổi giá trị $\theta$ như thế nào nhưng thuật toán gradient descent vẫn tìm được $\hat\theta$
* Giá trị $\theta$ được đặt khác nhau nhưng nhìn chung gradient descent trả về cho ta kết quả tương đối gần nhau

5. Thay đổi điều kiện dừng bằng cách sử dụng điều kiện `gradient` < `epsilon`. (Gợi ý: Các bạn khởi tạo biến `epsilon` có giá trị thật nhỏ bằng dòng lệnh, ví dụ `epsilon = 1e-5` tương đương với epsilion = 0.00001).
"""

def batchGradient_CONDITION(n_iterations, eta, epsilon, theta, X_b, y): 
  for iteration in range(n_iterations):
     gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) 
     if gradients.any() >= epsilon:
       break
     theta = theta - eta * gradients
  return theta

# Setting parameter
eta = 0.001 # learning rate
n_iterations = 1000
m = 100

for i in range(5): 
  epsilon = 10**(-i)
  theta = np.random.randint(100, size = (2,1)) # random initialize theta
  print('epsilon value:', epsilon)
  print("Theta value: ",theta)
  print("Gradient return: ",batchGradient_CONDITION(n_iterations,eta,epsilon,theta,X_b,y))

"""Nhận xét: 
* Mặc dù thay đổi epsilon giảm dần nhưng kết quả lại nhận được rằng $\theta = \hat\theta$

### Bài tập 4. Thuật toán Stochastic Gradient Descent
1. Cài đặt thuật toán Stochastic Gradient Descent cho bài toán Linear Regression trên dữ liệu `data-demo.xls` bằng cách sử dụng chương trình Python tại trang `125` và in ra màn hình kết quả của `theta`.

        n_epochs = 50
        t0, t1 = 5, 50 # learning schedule hyperparameters
        def learning_schedule(t):
            return t0 / (t + t1)
        theta = np.random.randn(2,1) # random initialization
        for epoch in range(n_epochs):
            for i in range(m):
                random_index = np.random.randint(m)
                xi = X_b[random_index:random_index+1]
                yi = y[random_index:random_index+1]
                gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
                eta = learning_schedule(epoch * m + i)
                theta = theta - eta * gradients
"""

m = 100
n_epochs = 50
t0, t1 = 5, 50

def learning_schedule(t):
    return t0/(t + t1)

theta = np.random.randn(2,1)

for epoch in range (n_epochs):
    for i in range (m):
        random_index = np.random.randint(m)
        # xi,yi already read from DataFrame 
        xi = X_b[random_index : random_index + 1]
        yi = y[random_index : random_index + 1]
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(epoch * m + i)
        theta = theta - eta * gradients

print(theta)

"""2. So sánh nhanh hai chương trình Python đã đề cập ở `Bài tập 3.1.` và `Bài tập 4.1.`.
* Ta nhận thấy rằng kết quả giữa Batch Gradient Descent và Stochastic Gradient Descent là tương đương nhau
* Gradient descent áp dụng phương pháp tính trung bình cho đạo hàm nhưng Stochastic lại không sử dụng phương pháp đó nhưng lại cho kết quả tương đương

3. Vai trò của hàm `learning_schedule` trong chương trình Python đã đề cập ở `Bài tập 4.1.`. Nếu không sử dụng hàm này thì kết quả của bài tập sẽ như thế nào.
"""

m = 100
n_epochs = 50
t0, t1 = 5, 50

theta = np.random.randn(2,1)
# Without learning schedule
for epoch in range (n_epochs):
    for i in range (m):
        random_index = np.random.randint(m)
        # xi,yi already read from DataFrame 
        xi = X_b[random_index : random_index + 1]
        yi = y[random_index : random_index + 1]
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
        eta = 0.01 # Self-choose for eta
        theta = theta - eta * gradients

print(theta)

"""Nhận xét:
* Không áp dụng learning_schedule ta vẫn có thể cho kết quả tương đương như Batch Gradient Descent tuy nhiên có điểm cần lưu ý là phải chọn giá trị $\eta$ sao cho hợp lí

4. Sử dùng lớp [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) để cài đặt thuật toán Stochastic Gradient Descent thay cho chương trình Python đã đề cập ở `Bài tập 4.1.`.
"""

from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter = 100, tol = 1e-3, penalty = None, eta0=0.1)
sgd_reg.fit(X,y)
sgd_reg.intercept_, sgd_reg.coef_

"""### Bài tập 5. Thuật toán Mini-Batch Gradient Descent
1. Cài đặt thuật toán `Mini-Batch Gradient Descent` cho bài toán `Linear Regression` trên dữ liệu `data-demo.xls` bằng cách `chỉnh sửa` chương trình Python tại trang `125` và in ra màn hình kết quả của `theta`.

        n_epochs = 50
        t0, t1 = 5, 50 # learning schedule hyperparameters
        def learning_schedule(t):
            return t0 / (t + t1)
        theta = np.random.randn(2,1) # random initialization
        for epoch in range(n_epochs):
            for i in range(m):
                random_index = np.random.randint(m)
                xi = X_b[random_index:random_index+1]
                yi = y[random_index:random_index+1]
                gradients = 2 * xi.T.dot(xi.dot(theta) - yi)
                eta = learning_schedule(epoch * m + i)
                theta = theta - eta * gradients
"""

eta = 0.1  
theta = np.random.randn(2,1)  
for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)    
    theta = theta - eta * gradients

def create_mini_batches(X, y, batch_size): 
    mini_batches = [] 
    data = np.hstack((X, y)) 
    np.random.shuffle(data) 
    n_minibatches = data.shape[0] // batch_size 
    i = 0
    for i in range(n_minibatches + 1): 
        mini_batch = data[i * batch_size:(i + 1)*batch_size, :] 
        X_mini = mini_batch[:, :-1] 
        Y_mini = mini_batch[:, -1].reshape((-1, 1)) 
        mini_batches.append((X_mini, Y_mini)) 
    if data.shape[0] % batch_size != 0: 
        mini_batch = data[i * batch_size:data.shape[0]] 
        X_mini = mini_batch[:, :-1] 
        Y_mini = mini_batch[:, -1].reshape((-1, 1)) 
        mini_batches.append((X_mini, Y_mini)) 
    return mini_batches

def miniBatchGradient(X, y, learning_rate = 0.1, batch_size = 32): 
    theta = np.random.randint(100, size = (2,1))
    max_iters = 3
    for itr in range(100): 
        mini_batches = create_mini_batches(X, y, batch_size) 
        for mini_batch in mini_batches: 
            X_mini, y_mini = mini_batch 
            gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
            theta = theta - learning_rate * gradients
    return theta

print(miniBatchGradient(X,y, 0.1, 32))

"""2. Sử dùng lớp [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) để cài đặt thuật toán Stochastic Gradient Descent thay cho chương trình Python đã đề cập ở `Bài tập 5.1.`. *Gợi ý*: Khác với yêu cầu trong `Bài tập 4.4.`, các bạn sẽ chia bộ dữ liệu huấn luyện ban đầu thành các bộ dữ liệu nhỏ có kích cỡ gọi là `Mini-Batch Size`, ví dụ các bạn có thể sử dụng $4$ chẳng hạn. Cuối cùng, thay vì các bạn sử dụng phương thức [`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor.fit) của [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) thì các bạn sẽ sử dụng phương thức [`partial_fit`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor.partial_fit) cho các bộ dữ liệu nhỏ thành phần."""

from sklearn import linear_model
print('Import linear model successfully')

miniSGD = SGDRegressor()

miniSGD.partial_fit(X,y)

miniSGD.intercept_, miniSGD.coef_